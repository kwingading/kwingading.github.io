---
draft: true
---

_Note: This is a review/critique of Zeng et al’s "Contested Chinese Dreams of AI? Public discourse about Artificial intelligence on WeChat and People’s Daily Online" (2020)_

# Introduction
Understanding public opinion traditionally required the use of polls and interviews. However, the rise of Big Data has paved the way for the use of novel analytical tools and methodological approaches. An example is Zeng et al’s study (2020), which investigates the Chinese public’s opinion on AI by analysing articles from the social medium WeChat. The study is motivated by literature on counterpublic spheres (Downey & Fenton, 2003), which argues that bottom
up criticism of mainstream discourse helps shape the implementation of scientific technologies (Gerhards & Scha ̈fer, 2009). The official narrative on AI is sketched out by performing the same analysis on People’s Daily, an official newspaper controlled by the CCP. The results from both analyses are then compared to assess the degree to which WeChat acts a counterpublic sphere. Zeng et al’s framework focuses on “standing, framing and positioning”; that is, it examines the breakdown of actors on WeChat engaging with the topic of AI, the thematic framings used when discussing AI and the sentiments harbored by various actors towards AI. Their framework is operationalized using topic modelling and sentiment analysis. This essay evaluates the various opportunities and limitations of Zeng’s study under an epistemological, socio-ethical and practical lens.

# Epistemological
‘Big Data’ is defined as “research that represents a step change in the scale and scope of knowledge about a given phenomenon (Schroeder, 2014).” Based on this definition, a good way to evaluate the epistemological opportunities of this study is to compare it to studies using previously-established methodologies. Traditionally, public opinion was collected via public opinion polls. This approach was costly and hence diffcult to scale. In contrast, Zeng’s Big Data approach accounts for a much larger sample size. An increased sample size is desirable as it leads to an increased power to detect smaller differences, a reduction in uncertainty by reducing the margin of errors and increasing the confidence intervals (though not always — as will be discussed below), and a better ability to account for complexity as a large sample size can afford to introduce more variables without building up too much noise.

Big Data approaches also have the potential to provide a more accurate depiction of public opinion as it overcomes the ‘response biases’ that arise in traditional polling methodologies. Examples of such biases include the respondent’s desire to present themselves as ‘socially desirable’ by giving more ‘acceptable’ responses (Furnham, 1986; Wiseman, 1972) and the potential of question phrasing to alter a respondent’s answer (Graham & Howard, 1982). Big Data approaches bypass such biases by directly analysing user-generated data ‘in the wild’. However, the non-specially-curated nature of the data introduces new problems in data processing and measurement, which will be addressed below.

Although Big Data approaches have numerous advantages over previous approaches, they also have their limitations. To begin, its replacement of old tools and approaches has led to the discounting of smaller but specially-curated datasets. This is problematic as the quality of data matters more than the quantity, with such datasets yield insights that a bigger but more coarse dataset would fail to show. This is illustrated in the public opinion polls running up to the 1936 presidential election. The Literary Digest poll garnered 2.4 million responses and wrongly predicted that Landon would have a landslide win over Roosevelt. Conversely, the poll run by the American Institute of Public Opinion, which had a sample size of 2% that of the LD poll, “predicted the outcome of the election within 1% of the actual result” (Kaplan et al, 2014). This is due to the issue of population bias, which is also observed in Zeng’s study.

Although Zeng’s study claims to be investigating “public opinion”, it fails to take the entire Chinese public into account proportionately. This limitation arises due to sampling bias, which places huge emphasis on certain demographics while neglecting others. At first blush, the scope of the Chinese public is already limited to those active on WeChat. This is problematic as active members of WeChat posting about AI are unlikely to be representative of the entire Chinese population. For example, as of 2020, 45.5% of the Chinese population still do not use the internet (Lin, 2020) and are hence not represented in the study. This has ramifications for generalisability as previous studies have illustrated opinion divides between the Chinese rural and urban populations. For instance, Kostka’s study shows how there is a lower approval of China’s Social Credit System by Chinese locals living in rural areas and those with a lower income (Kostka, 2019).

Setting aside the issue of demographic bias, it is unclear that Zeng’s study even fo- cuses on ‘individuals’, which are typically conceptualised as the main fundamental units making up the public. Zeng et al have observed how WeChat is composed primarily of institutional actors: out of all the accounts used in their study, only 30.7% of accounts are labelled as non-institutional actors and 3% as ‘other’; actors from industry make up a whopping 46.2% and on average post the most AI-related articles per account than any other account category. The definition of public opin- ion centers around the beliefs and attitudes of individuals in a community. Although institutions are made up of the activities of individuals, “they are not reducible to them” (Hodgson, 2006); the opinion of individuals in an institution will hence not be completely aligned with that of the institution. For example, industry actors are primarily driven by financial incentives. In the case of China, this is tangled up with maintaining good relations with the State due to the authoritarian nature of the political system. Indeed, by 2016, “68% of China’s private companies [already] had party bodies” (McGregor. 2019). Industry actors will hence be forced to express opinions that are aligned to the State regardless of what individuals constituting such actors believe, making the presence of a counter-culture unlikely. This suggests that the aspect of social reality captured by Zeng’s study is not what they initially set out to explore and links to the wider question of “who constitutes the ‘public’ being measured”. This is termed more broadly as “construct validity”, which begs the question of whether one’s data and measurements really measure what they think they measure. This is especially important in this case as the construct of ‘public opinion’ is latent and hence must be operationalised via observed attributes such as words and phrases (Oltneau, 2019).

Another epistemological challenge of this study is the ‘measurement validity’ of their approach. Olteneau argues that the software used for data processing can introduce measurement errors (2019). Indeed, there are limitations to using sentiment analysis for gauging public sentiment. Sentiment analysis refers to the use of automatic machine evaluation for sentiment annotation. The metric used in Zeng’s study is the ‘Relative Risk’ value, which “equals the number of positive words [present] divided by the number of negative words” (2020). I argue that the results produced using said methodology is a crude approximation of human-based sentiment identification. Firstly, Zeng’s approach suffers from a lack of nuance. It glosses over the varying strength of sentiments and simplifies the possible existence of contrasting emotions. Their approach also operates under the assumption that the sentiment behind an article can be gathered solely from explicitly-stated valence words. This is a fallacious heuristic to follow; in day-to-day life, negative sentiments can be expressed even without the explicit use of negative-sentiment-laden terminology. Semantics and Pragmatics literature in Linguistics have long grappled with such a problem whereby the communicated meaning often cannot be wholly derived from the logical form of the sentence. As highlighted by Jaszcolt’s ‘Default Semantics’, inferred meaning arises from other factors outside of word meaning and sentence structure; some examples include “world knowledge”, “situation of discourse” and “stereotypes” (Jaszcolt, 2005). Thus, analysing sentiment using only word meaning would result in an inaccurate model. Indeed, Beasley and Mason (2015) have previously shown how sentiment analysis using term frequency is a coarse measure for actual user sentiment. This issue is exacerbated by the high levels of censorship in Chinese cyberspace. Fear of persecution will force writers to communicate unorthodox ideas through the use of rhetorical techniques such as irony, which inverts the ‘typical sentiment’ associated with a word. This will subsequently require readers to “read between the lines” in order to garner the full scope of meaning intended by the writer (Melzer, 2014; Strauss, 1952). Moreover, what is omitted is potentially even more important than what is present, as shown by how socioethical discussions of AI with a positive sentiment are largely missing in Zeng’s study. To validate the conducted automatic sentiment analysis, Zeng’s study compared 500 of the results to human annotations and deemed the correlation between the two variables as “statistically significant”. However, the Pearson correlation coefficient reported was 0.238, which is classified as “weak” (Akoglu, 2018). Furthermore, it is unclear how high-quality the human annotations are. This, along with our previous discussion, suggests the limited ability of sentiment analysis to capture public sentiment.

Other specific methodological challenges are also encountered in data querying and data aggregation. Data querying is very important as the keyword choice greatly shapes the resulting dataset. In Zeng’s study, their data is queried by searching for articles which feature the keyword '人工智能'(AI). The use of a single keyword greatly limits the scope of included pieces, potentially omitting relevant articles. Indeed, alternative words to 人工智能 are used by the Chinese public to refer to the same concept. A Baidu search yields hits where the English abbreviation of ‘AI’ is used; there also exists articles about AI that actively refer to the technology as ‘robots’. The latter group of articles are arguably written by a less ‘scientifically-informed’ demographic, which suggests how articles written by certain demographics could have been left out. When aggregating data by topics, more weight will naturally be assigned to highly-active users due to the larger amount of information they produce. Zeng et al fail to address this asymmetry and hence give a disproportionate amount of weight to the most active accounts, which according to their analysis, are those of institutional actors. Furthermore, various NLP studies have suggested the need for addressing identical articles that could be posted by various institutional accounts or bots. A related issue that occurs in Chinese cyberspace includes ‘insincere posts’. A study has found how the Chinese state pursues a strategy of “active censorship” where workers are paid to post articles and comments supporting the CCP (King et al, 2017).
To conclude this section, although Big Data methods offer opportunities over traditional polling methods, there remain epistemological challenges concerning construct validity and measurement validity.

# Socio-Ethical
One of the main conclusions arrived at by Zeng et al is that the discussion of AI-related ethical and social issues are fading in China despite the increasing need for such discourse to take place. The authors claim that this is problematic as such discussions foster a counterpublic sphere. The counterpublic sphere then, in turn, serves as a counterbalance that prevents technological innovation from being driven solely by political and economic interests, without regard for socio-ethical consequences. In the paper, the authors bring up the recent CRISPR babies scandal as an example of ‘unchecked’ technological innovation. Thus, an implication of a lack of a counterpublic sphere is a potential disregard for AI-related socio-ethical issues, such as algorithmic fairness and privacy. The consequences of this neglect are not just localised to China, but could in fact affect the global landscape and trajectory of AI development. Firstly, as China is now growing into an ‘AI superpower’ (Lee, 2018), it therefore plays a role in shaping the global standard of how AI development should be approached and which key issues should be considered. Another point is that countries are already incentivised to underinvest in safety and socio-ethical dimensions due to the first-mover advantage in terms of both profits and geopolitical power. A lack of regard for socio-ethical issues from China could therefore trigger a potential “race to the bottom” on socio-ethical dimensions in the development of AI globally (Askell, 2019).

Although Zeng et al’s conclusion above points to worrying socio-ethical implications, it is important to take a step back and assess how accurate their claims are. They primarily attribute this decline in socio-ethical discussion to the widespread ‘AI hype’ present in China. Their suggested hypothesis is supported by their findings on how AI-related discussions are predominantly scientific and economic in nature. Other external findings, such as China ranking first in the world in AI-related patent filings and publications (Koltz, 2019), also lend credence to their hypothesis. Moreover, unlike its Western counterparts, China has not yet experienced an AI Winter that can serve as a realistic reminder and a potential hype-deflating mechanism. However, Zeng’s conclusion is primarily driven by evidence that “discussion of socio-ethical topics [regarding AI] has sharply decreased on. . . [WeChat] in recent years” (2020) where they presume that the lack of discussion is a sign of public complacency. It is hence unclear that their conclusions can extend beyond ‘public-facing’ social media. Indeed, the authors seem to make a leap in reasoning, going from the more specific finding “that social media’s role as a counterpublic sphere in AI discourses is minimal” to the more general claim that there is “a lack of a counterpublic sphere” championing AI-related socio-ethical issues. This is problematic as 1) they have not explored other possible reasons for the lack of ‘dissenting voices’ in WeChat and 2) the possibility of the counterpublic sphere manifesting differently.

Delving deeper into the first counterargument, their study fails to touch on the selection effect which arises, whereby members with dissenting views systematically avoid public engagement with social media. This selection effect is primarily due to the high levels of online censorship found in China since the regulatory shifts from 2012 to 2014; indeed, when Xi came into power in 2012, the infamous ‘Document 9’ was distributed amongst party leaders with clauses that featured calls to “strengthen guidance of public opinion on the Internet [and] purify the environment of public opinion on the Internet” (Creemers, 2016). A specific example of ‘counter-culture-ousting’ can be observed with the feminist movement in China: during the government’s recent crackdown on social media, the prominent feminist account Feminist Voices was expelled from WeChat and Weibo in 2018 (Lu & Chao, 2019). Although Zeng et al have acknowledged the possibility of controversial posts being deleted, they fail to account for the invisible forces acting on behaviour — put simply, the high likelihood that an article would never be posted in the first place due to costly risks associated with posting ‘non-State-aligned’ content. For instance, Chinese law stipulates that “if one user’s post is considered defamatory and is viewed 5,000 times or reposted more than 500 times, the original poster can face a jail sentence.” (Lu, 2020). As the collected data is situated in a larger contextual and institutional landscape, it must be treated as such during analysis. In this case, the political backdrop of China needs to be considered as the authoritarian state’s huge censorship apparatus and related systems of punishment greatly reduce the likelihood of anyone publicly sharing ‘unorthodox’ opinions online. This makes the study of social media unsuitable to the task of ‘public opinion mining’ in China. In contrast, traditional social science approaches are more suitable due to 1) its receptiveness to subtleties that go undetected by coarse language models and 2) the more ‘personalised’ methods used (especially interviews) which would yield more sensitive information. Thus, the declining AI-based socio-ethical discussion on WeChat can at least be partly attributed to the ever-tightening grip the CCP has over public dissenting voices. This suggests that a counterpublic sphere, if one exists, is most likely to lie elsewhere. One caveat to the counterargument is that the CCP censorship apparatus mainly focuses on topics that weaken state legitimacy (Carlson, 2013). The general topic of AI does not appear to fit under this umbrella as it is typically non-political. Although feminism is not an obvious candidate for being politically sensitive, it is still a social issue. Conversely, AI is a technology. However, there are a few subjects involving AI that are politically sensitive; examples include both the potential and existing application of AI technologies, such as the use of facial recognition technologies for surveillance in XinJiang (Leibold, 2020). Moreover, as the development of AI technologies becomes more central to China’s consolidation of geopolitical power on the international stage, any criticism that potentially hinders further development could be perceived as a threat.

This brings us to our second counterargument to Zeng et al’s claim: the possibility of a counterpublic manifesting differently. Zeng et al draw comparisons between the lack of resistance in China and the very vocal pushback encountered in Europe and US by activists and scholars with regards to AI-related socio-ethical issues. By doing so without stating caveats, it reveals their underlying assumption that the counterpublic will manifest in the same public and vocal manner in China. This assumption is too simplistic as it fails to account for context. As outlined above, the Chinese political system and censorship apparatus provides a vastly different backdrop to that found in the US and EU (Liu, 2020). A contextually-sensitive hypothesis is that discussion of socio-ethical topics must be brought into the private sphere —and perhaps even offine— in order to evade punishment. Calls for change may also manifest in different ways other than outright public resistance; examples include making connections and doing advisory work, and thoughtful framing and communication of socio-political ideas under the guise of other dimensions such as ‘economic’ potential. This is clearly observed in China’s Feminist movement where a lot of work happens behind-the-scenes, from starting petitions, filing lawsuits, and “liais. . . [ing] with gender scholars and researchers affiliated with universities or government institutions to push for improvement or passage of policies”. Prominent feminist figure Lu Pin describes how “they carefully chose topics they believed they could bring to public attention without triggering a backlash from the government”. Zhao Ming Yang, a Social Movement sociologist from the University of Pennsylvania, outlines how the building of social movements in China poses a stark contrast to that in the US; the latter consists of marches, resistance and loud fanfare, whereas for the former, every step is careful and calculated due to the very politicised environment (Lu & Cao, 2019). This, however, engenders the question of whether this more ‘subtle’ counterculture that arises due to cultural and political context can be considered as a counterpublic. A counterpublic is defined as an alternative public sphere that allows for people to “come together to formulate and circulate counter-discourses, in order to contest and alter dominant public discourses” (Holm, 2019). Thus, although the networked communities and behind-the-scenes work keeps the necessary ‘counter-discussion’ going and acts as a force of change, it does not fulfill all the functions of a counterpublic. More specifically, by moving away from the public eye, it is dfficult to gather more support and momentum, with discussion becoming fairly limited to a limited group of members rather than being open to public contribution.

At this point, it is therefore important to disentangle two closely-related claims that Zeng et al seem to conflate: 1) the declining discussion on AI-related socio-ethical topics and public complacency and 2) the lack of a strong, vocal counterpublic that is very much in the public eye. Given our previous discussion, the first claim is tenuous whilst the second seems to hold to some degree, notably in online contexts, but lacks the nuance that addresses other context-sensitive manifestations. With regards to the first point, other metrics provide evidence that citizens do engage with socio-ethical AI topics. For example, during the first two days after the release of the Beijing AI Principles, netizens looked up the document more than five million times on the Chinese search engine Baidu (夏志堅, 2019). It is important to highlight that this event occurred in 2020 whereas the data sampled by Zeng’s study ranges from 2015-2018. Over the past two years, there has been a sudden shift in focus towards AI ethics by Chinese academia, industry and the State. Examples include the outpouring of AI principles being published, such as the ‘Principles of Next-Generation Artificial Intelligence Governance’, Beijing AI Principles and China’s Artificial Intelligence Industry Alliance (AIIA) released in May and June of 2019 (Laskai & Webster, 2019). It would hence be interesting to investigate whether AI-related socio-ethical discussion has increased since Zeng et al’s study, what the sentiment underlying such discussions is and how unanimous the voices are. Despite the recent uptick of interest in AI socioethics, there have still not been any signs of a strong counter-discourse circulating. As Zeng et al point out, academia, industry and the State all seem to be aligned with and take part in the ‘mainstream’ socio- ethical AI discussion.

To conclude, although the authors have explicitly stated the socio-ethical implications of their research, the more general claims regarding declining socio-ethical discussion and to some extent the lack of a counterpublic made cannot be derived from just the findings in their study. Although public complacency is a likely factor, they fail to take into account alternative hypotheses and potential confounding factors. To back up their claim, they need to account for the ‘selection’ effect and consider a culturally and socio-politically sensitive manifestation of counterpublic spheres. For instance, it would be interesting to explore the possibility of a Chinese Counterpublic movement being fostered abroad. This is seen in the case of Feminism and the more historic case of anti-Qing revolutionaries à la Sun Yat Sen (Lu & Cao, 2019).

In response to the claim that there is a lack of a counterpublic on socio-ethical AI issues on Chinese social media, the authors propose several solutions. The first is to put the responsibility of cultivating a counterpublic sphere on researchers in the social sciences and humanities. Once again, this suggestion stems from parallel movements seen in the West. It is, however, unclear if this same path would be effective in China. Chinese academia, and notably the fields of the social sciences and humanities, are regulated and monitored tightly with recent crackdowns (Phillips & Pilkington, 2016) as the conclusions they draw could serve as a potential threat to the CCP. An alternative I propose is for scientists and engineers to emphasise the increased performance advantages relating to more ‘objective’ dimensions, such as accuracy and safety, were socio-ethical considerations to be built into technological systems. Zeng et al also suggest that the Chinese government may be incentivised to focus on ethics as a way of building trust with Western countries following the recent tech trade war. However, China’s recent actions hint at a different approach being taken, in which they focus on building their own tech base to allow for technological independence. This is observed in Xi’s call to return to foundational research to establish a “solid foundation” (Kania & Creemers, 2018) and the formation of a “national integrated circuit industry investment fund” to reduce dependence on foreign chips, an area that China has previously neglected (Allen, 2019). It is hence possible that the ‘trade war’ has evoked the opposite reaction in China, acting as a catalyst for the government to ensure that AI technologies can be developed fully locally.

# Practical
The practical opportunities of the study include its academic value. Zeng’s study yields insights such as China’s strong belief in the economic potential of AI and the huge national focus placed on scientific developments in the field. However, the opportunities offered by this research appear to not be what was initially envisioned. As Zeng et al’s analysis focuses disproportionately on institutional actors with strong ties to the State, their results are closer to a reflection of what the Chinese elite wants the public to believe, which allows us to draw hypotheses about the State’s hold over different actors. For example, Civil society’s emphasis on the economic and political frame and lack of engagement with socio-ethical issues suggests a tight hold over such actors by the CCP. Furthermore, the general declining discussion around socio- ethical themes in recent years potentially reveals the growing importance of AI to the CCP in ensuring legitimacy and power. It is, however, important to note that certain findings in Zeng’s study could still be aligned with public opinion and hence offer insights on the subject. This can be ascertained by comparing their findings with evidence outside of their study. For instance, the widespread positively-charged discussion of the economic and technological frames of AI is most likely an accurate reflection of the presence of the AI hype in China as it is supported by ‘external’ evidence such as the outpouring of resources and investment into AI development since AlphaGo’s defeat of KeJie (Schiavenza, 2018). Thus, the academic value of this study ranges from Foreign Policy, China Studies to Sociology. Zeng’s study also contributes to public sphere literature as it provides empirical evidence that sug- gests how counterpublic spheres could manifest differently in different socio-political contexts, cultivating an appreciation for cultural differences. This could ultimately inform future approaches to public opinion mining in Authoritarian regimes.

Although this study was conducted in an academic setting, its practical limitations are more far-reaching. Public opinion mining is also traditionally used by governments to shape public policy. Indeed, a study examining multiple relevant case studies to study the relationship between public opinion and public policy inferred a likely causal relationship from the former to the latter (Burstein, 2003). Thus, the insights gleaned in academia are also of great use to governments. In this case, the CCP could use insights from Zeng’s study to shape AI policy. For example, they could respond to the public’s excitement for AI’s economic potential by offering more grants to industry or academic labs to innovate applications of AI technologies that would boost the economy.

However, public opinion results are a double-edge sword; the same knowledge that prompts democratic states to enact positive change can be used by authoritarian regimes to identify and capture dissenters. A practical challenge posed by Zeng’s study hence involves its potential role in enhancing surveillance and manipulation. There are numerous examples of the CCP’s use of technology for surveillance (McDonell, 2019), an example being the 2015 “Sharp Eyes” plan, which capitalises on video cameras and facial-recognition technology for widespread surveillance (Qiang, 2019). At first blush, the potential of increased effectiveness in surveillance from this study itself is very low. As mentioned in the previous section, the topic of AI is relatively apolitical, with limited findings from this study being obviously useful for surveillance or manipulation. Furthermore, the study itself has little new information on potentially sensitive socio-ethical issues other than its declining discussion on social media. However, I argue that the effects of such studies tend to be cumulative and that the declining discussion on socio-ethical issues is a potential symptom of such an effect. As an increasing number of publications in academia and industry look at public opinion using such technologies, the knowledge of potential widespread surveillance on social media enabled by ‘data mining’ tools along with the already very elaborate Chinese censorship apparatus could deter dissenting voices from speaking out on such platforms, notably those in Academia who are aware of such developments.

In terms of manipulation, the results from public opinion mining can enhance the effectiveness of the State’s persuasive tools by providing relevant information on propaganda framing and target groups. This would buttress the State’s manipulation prowess and allow them to bolster support for their favored policies (Dionne & Mann, 2003). For instance, the State could learn that the public is very excited by the economic potential of AI, and in turn, engage in active censorship by flooding the public with such information to increase the hype and drown out critical voices.

This highlights the increased potential for effective manipulation made possible by the extensive and far-reaching knowledge excavated with novel data mining tools. Again, this implication is not a result of Zeng’s study alone, but rather an accumulated number of studies. More abstractly, having this “deterministic knowledge” at hand lends the state a certain level of “omniscience” (Schroeder, 2014), ultimately undermining the fundamental values of human autonomy and free will. There have already been instances of ongoing manipulation enabled by Big Data in China. The nation’s various Social Credit Systems are an example. The social credit systems give users a score based on observed financial, social and moral behaviour; the score of the users would then enable or prevent them from accessing certain resources or obtaining certain privileges, thereby manipulating their behaviour by changing their incentive structures (Mitchell & Diamond, 2018). Existing examples include Alibaba’s Sesame Credit, which primarily determines one’s score using mobile payment transactions done via Alipay (Campbell, 2019). In a 2014 plan, the Chinese government has also outlined plans for a single citizen score which is calculated using a variety of data made available from Big Data, such as grocery shopping data (Hawkins, 2017).

Although the act of understanding public behaviour and subsequent “manipulation” sounds sinister to many, it does not always go hand-in-hand with malicious intent and can bring about certain positive developments. For example, through an understanding of public behaviour, China was alerted to widespread gaming addiction in minors and consequently adopted a strong stance to combat the issue (“Video game addiction”, 2019). Many Chinese citizens also view the social credit systems as a positive development due its capacity in helping build social trust and maintain harmony in society (Hawkins, 2017). It is, however, important to note that the above is a speculative hypothesis of potential impact. A more productive, but also more dicult, direction would be to gauge the actual extent to which such studies would enable more effective manipulation. Schroeder proposes that this requires figuring out the number of people who engage with the manipulative content and the purpose of manipulation. Intuitively, it is unclear that Zeng et al (2020) and related studies would exacerbate the situation as the CCP has already crafted an elaborate censorship apparatus involving social media since 2012 as previously outlined, potentially even involving the same technologies. An extensive answer to this would require a further survey of case studies, which is outside the scope of this essay.


# Conclusion
Zeng’s (2020) study presents a novel Big Data approach to studying public opinion. Although it yields certain interesting insights and offers certain epistemological opportunities over previous methodologies, there remain limitations regarding issues of manipulation, construct and external validity, and methodology. The socioethical implications of said study also feature weaknesses that need to be revised.


# Bibliography
Allen, G. (2019, February 6). Understanding China’s AI Strategy. Center for a New American Security. https://www.cnas.org/publications/reports/understanding-chinas- ai-strategy
Akoglu, H. (2018). User’s guide to correlation coefficients. Turkish Journal of Emergency Medicine 18(3): 91-93.
Askell, A., Brundage, M., & Hadfield, G. (2019). The role of cooperation in respon- sible AI development. arXiv preprint arXiv:1907.04534.
Bagdouri, M., Oard, D. W. (2015). “On predicting deletions of microblog posts,” in Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM ’15 (New York, NY: ACM), 1707–1710.
Burstein, P. (2003). The Impact of Public Opinion on Public Policy: A Review and an Agenda. Political Research Quarterly, 56(1): 29-40.
Campbell, C. (2019). How China Is Using Big Data to Create a Social Credit Score. Time. https://time.com/collection/davos-2019/5502592/china-social-credit-score/
Carlson, B. (2013, June). 7 things you can’t talk about in China. The World. https://www.pri.org/stories/2013-06-03/7-things-you-cant-talk-about-china
Creemers, R. (2017) Cyber China: Upgrading Propaganda, Public Opinion Work and Social Management for the Twenty-First Century. Journal of Contemporary China, 26(103): 85-100, DOI: 10.1080/10670564.2016.1206281
Diamond, A. M., Larry. (2018, February 2). China’s Surveillance State Should Scare Everyone. The Atlantic. https://www.theatlantic.com/international/archive/2018/02/china- surveillance/552203/
Dionne, E.J., Mann, T. (2003). Polling & Public Opinion: The good, the bad, and the ugly. The Brookings institution. https://www.brookings.edu/articles/polling- public-opinion-the-good-the-bad-and-the-ugly/
Downey, J., & Fenton, N. (2003). New media, counter publicity and the public sphere. New Media & Society, 5(2): 185–202. https://doi.org/10.1177/1461444803005002003
Furnham, A. (1986). Response bias, social desirability and dissimulation. Person- ality and Individual Differences, 7(3): 385-400.
Gerhards, J., & Sch ̈afer, M. S. (2009). Two normative models of science in the public sphere: Human genome sequencing in German and US mass media. Public Under- standing of Science, 18(4), 437– 451. https://doi.org/10.1177/0963662507082891
Hawkins, A. (2017, May 24). Chinese Citizens Want the Government to Rank Them.
Foreign Policy. https://foreignpolicy.com/2017/05/24/chinese-citizens-want-the-government- to-rank-them/
Hodgson, G. (2006). What Are Institutions? Journal of Economic Issues, 40(1), 1-25. Retrieved November 10, 2020, from http://www.jstor.org/stable/4228221
Holm, M. (2019). The Rise of Online Counterpublics? The limits of inclusion in a digital age. Uppsala Universitet.
Jaszcolt, K. (2005). Default Semantics: Foundations of a compositional theory of acts of communication. Oxford: Oxford University Press.
Kalton, G., Schuman, H. (1982). The Effect of the Question on Survey Responses: A Review. Journal of the Royal Statistical Society. Series A (General). 145(1): 42-73 (32 pages) 10.2307/2981421
Kania, E., & Kreemers, R. (2018, November 5). Xi Jinping Calls for ‘Healthy Devel- opment’ of AI (Translation). New America. http://newamerica.org/cybersecurity- initiative/digichina/blog/xi-jinping-calls-for-healthy-development-of-ai-translation/
Kaplan, R. M., Chambers, D. A., & Glasgow, R. E. (2014). Big data and large sample size: a cautionary note on the potential for bias. Clinical and translational science, 7(4), 342–346. https://doi.org/10.1111/cts.12178
King, G., Pan, J., Roberts, M. (2017). How the Chinese Government Fabricates Social Media Posts for Strategic Distraction, not Engaged Argument. American Political Science Review, 111(3): 484-501.
Koltz, F. (2019, November 8). Is China Taking the Lead in AI? MIT Sloan Review. https://sloanreview.mit.edu/article/is-china-taking-the-lead-in-ai/
Kostka, G. (2019). China’s social credit systems and public opinion: Explaining high levels of approval. New media & Society. 21(7): 1565–1593.
Laskai, L., & Webster, G. (2019, June 17). Translation: Chinese Expert Group Of-
fers ‘Governance Principles’ for ‘Responsible AI’. New America. http://newamerica.org/cybersecurit initiative/digichina/blog/translation-chinese-expert-group-offers-governance-principles- responsible-ai/
Lee, Kai-Fu. (2018). AI Superpowers: China, Silicon Valley, and the New World Order. Houghton Mifflin Co., USA.
Lin, W. (2020, April 28). China’s internet users reach 900 million, live-streaming
ecommerce boosting consumption: report. Global Times. https://www.globaltimes.cn/content/1187
Lu, S., Chao, M. (2019, August 28). Thwarted at Home, Can China’s Feminists Rebuild a Movement Abroad? ChinaFile. https://www.chinafile.com/reporting- opinion/postcard/thwarted-home-can-chinas-feminists-rebuild-movement-abroad.
Lu, S. (2020, October 22). Kicked off Weibo? Here’s what happens next. Rest of World. https://restofworld.org/2020/weibo-bombing/
McDonell, S. (2019, June 7). China social media: WeChat and the Surveillance State. BBC News. https://www.bbc.com/news/blogs-china-blog-48552907
McGregor, R. (2019, July 25). How the state runs business in China. The Guardian. https://www.theguardian.com/world/2019/jul/25/china-business-xi-jinping-communist- party-state-private-enterprise-huawei
Melzer, A. (2014). Philosophy Between the Lines: The Lost History of Esoteric Writing. Chicago: The University of Chicago Press.
Nafziger, R. (1946). EM 4: Are Opinion Polls Useful? https://www.historians.org/about- aha-and-membership/aha-history-and-archives/gi-roundtable-series/pamphlets/em- 4-are-opinion-polls-useful-(1946)/do-polls-form-public-opinion
Olteanu, A., Castillo, C., Diaz, F., Kiciman, E. (2019). Social Data: Biases, Method-
ological Pitfalls, and Ethical Boundaries. Frontiers in Big Data. https://doi.org/10.3389/fdata.2019.0
Page, B. (1994). Democratic Responsiveness? Untangling the Links between Public Opinion and Policy. PS: Political Science and Politics, 27: 25-29.
Phillips, T., & Pilkington, E. (2016, May 24). No country for academics: Chinese
crackdown forces intellectuals abroad. The Guardian. http://www.theguardian.com/world/2016/ma china-crackdown-forces-intellectuals-abroad
Qiang, X. (2019). The Road to Digital Unfreedom: President Xi’s Surveillance State. Journal of Democracy, (1), 53-67.
Schiavenza, M. (2018, September 25). China’s ’Sputnik Moment’ and the Sino-
American Battle for AI Supremacy. Asia Society. Retrieved from: https://asiasociety.org/blog/asia/c sputnik-moment-and-sino-american-battle-ai-supremacy.
Schroeder, R. (2014). Big Data and the brave new world of social media research. Big Data & Society. https://doi.org/10.1177/2053951714563194
Strauss, L. (1952). Persecution and the Art of Writing. Chicago: University of Chicago Press.
Video game addiction: China imposes gaming curfew for minors. (2019, November 6). BBC News. Retrieved from: https://www.bbc.co.uk/news/world-asia-50315960
Wise, A., Shaffer, D. (2015). Why theory matters more than ever in the age of big data. Journal of Learning Analytics, 2(2): 5–13
Wiseman, F. (1972). Methodological Bias in Public Opinion Surveys. The Public Opinion Quarterly. 36(11): 105-108.
Zeng, Ji., Chan, Ch., Scha ̈fer, M. (2020): Contested Chinese Dreams of AI? Public discourse about Artificial intelligence on WeChat and People’s Daily Online. Information, Communication & Society, DOI: 10.1080/1369118X.2020.1776372
夏志堅. (2019, November 11). AI的倫理治理挑戰：中美歐日英各方專家怎麼看？ Tencent News. https://new.qq.com/omn/20191101/20191101A0ALA400.html
